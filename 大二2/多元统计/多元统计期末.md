多元正态：$f(x)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp\{-\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu)\}$

### 因子分析(FA)

$X-\mu_{(p\times1)}=L_{(p\times m)}F_{(m\times 1)}+\varepsilon_{(p\times1)}$

$E(F)=0$，$Cov(F)=E(FF')=I_{(m\times m)}$，$E(\varepsilon)=0$，$Cov(\varepsilon)=E(\varepsilon\varepsilon')=\Psi_{(p\times p)}$，$Cov(\varepsilon,F)=E(\varepsilon F)=0_{(p\times m)}$，$\Psi$为对角阵

$\Sigma=Cov(X)=E(X-\mu)(X-\mu)'=LL'+\Psi$$\to$$\sigma_{ii}=\sum\limits_{k=1}^{m} l_{ik}^2+\Psi_i=h_i^2+\Psi_i$，$\sigma_{ik}=Cov(X_i,X_k)=l_i'l_k=\sum\limits_{j=1}^{m}l_{ij}l_{kj}$

$Cov(X,F)=E[(X-\mu)F']=L$$\to$$Cov(X_i,F_j)=l_{ij}$

当$m<<p$时，无法进行FA

$L$和$F$不唯一

对变量的拉伸不影响FA的结果，即不影响$F$（$X$左乘对角阵$C$）

PCA方法进行FA：当最后的$p-m$个特征值很小，则可忽略其在整体方差中的贡献，有$\widetilde{L}=[\sqrt{\hat\lambda_1}\hat e _1:\sqrt{\hat\lambda_2}\hat e _2:...:\sqrt{\hat\lambda_m}\hat e _m]$，$\widetilde{\Psi}=S-\widetilde{L}\widetilde{L}'$，误差$S-(\widetilde{L}\widetilde{L}'+\widetilde{\Psi})$的元素平方和小于等于$\sum\limits_{i=m+1}^{p}\hat\lambda_i^2$（未加上特征值平方和），该方法一般性因子所能解释的方差比例$\frac{\sum\limits_{i=1}^m\hat\lambda _i}{tr(S)}$，因为$h_i^2=\hat\lambda_i$（额外假设$L$列正交，$\Psi$很小）

MLE方法进行FA：认为一般性因子和特殊因子都是正态分布，即$F\sim N_m(0,I)$，$\varepsilon\sim N_p(0,\Psi)$，且二者独立，还要满足$L'\Psi^{-1}L=\Delta$。计算时通过$\Sigma=LL'+\Psi$联系到最大似然式子中，当进行标准化时，有$\rho=V^{-\frac{1}{2}}\Sigma V^{-\frac{1}{2}}$，则相应地$L_z=V^{-\frac{1}{2}}L$，$\Psi_z=V^{-\frac{1}{2}}\Psi V^{-\frac{1}{2}}$。（额外假设：多元正态）

第j个一般性因子对方差的贡献。。。。。

旋转可以提高因子的可解释性。Varimax旋转标准：拉伸$(\widetilde{l}_{ij}^*)^2=(\hat l_{ij}^*)^2/\hat h_i^2$，选择旋转，使得$V=\frac{1}{p}\sum\limits_{j=1}^m\{\sum\limits_{i=1}^p(\widetilde{l}_{ij}^*)^4-[\sum\limits_{i=1}^p(\widetilde{l}_{ij}^*)^2]^2/p\}$。估计因子得分：认为$L$和$\Psi$已知，计算方法有WLS(加权最小平方)、OLS(一般最小平方)和regression(回归)

WLS：$\sum\limits_{i=1}^p\frac{\varepsilon^2_i}{\Psi_i}=\varepsilon'\Psi^{-1}\varepsilon=(x-\mu-Lf)'\Psi^{-1}(x-\mu-Lf)$最小，可得$\hat f=(L'\Psi^{-1}L)^{-1}L'\Psi^{-1}(x-\mu)$（Bartlett分数）（$\mu$、$L$、$\Psi$为根据样本估计值）（标准化不影响得分估计）（该估计是无偏的）

OLS：$\sum\limits_{i=1}^p\varepsilon^2_i=(x-\mu-Lf)'(x-\mu-Lf)$，可得$\hat f=(L'L)^{-1}L'(x-\mu)$，其中$\hat f_j=\frac{1}{\sqrt{\lambda_j}}e_j'(x-\mu)=\frac{1}{\sqrt{\lambda_j}}y_j$（PC方法下）

Regression：认为$X\sim N_p(\mu,LL'+\Psi)$，则$(X-\mu,F)\sim N_{p+m}(0,\left[\begin{matrix}  LL'+\Psi & L  \\ L' & I_m\end{matrix}\right])$，故而$E(F|x)=L'\Sigma^{-1}(x-\mu)=L'(LL'+\Psi)^{-1}(x-\mu)$，故而$\hat f_j=\hat L'\hat \Sigma^{-1}(x_j-\bar x)=\hat L'(\hat L\hat L'+\hat \Psi)^{-1}(x_j-\bar x)$（如果采用PC方法得到的$L$和$\Psi$，则$\hat f_j=\hat L'S^{-1}(x_j-\bar x)=(P_m\Lambda_m^{1/2})'(P\Lambda^{-1}P')(x_j-\bar x)=\Lambda_m^{-1/2}P_m'(x_j-\bar x)$，与PC得分有关）

**在MLE方法下得到的$L$和$\Psi$无论是否标准化都不影响得分**

$\hat f_j^{WLS}=(I+(\hat L'\hat \Psi^{-1}\hat L)^{-1})\hat f_j^{R}$，$E(\hat f_{j}^{WLS}|F)=(L'\Psi^{-1}L)^{-1}L'\Psi^{-1}LF=F$，WLS方法得到的估计的方差为$(L'\Psi^{-1}L)^{-1}$，R方法得到的估计的方差为$(I+L'\Psi^{-1}L)^{-1}$更小

发生旋转时，估计值仍满足$\hat L^*=\hat L T$，$\hat f_j^*=T'\hat f_j$



### 典型相关分析(CCA)

找到一组$a$和$b$，使得维度数为$p$和$q$的两个随机变量的$corr(a'X,b'Y)=corr(U,V)$最大，$\max\limits_{a,b\neq 0}\frac{a'\Sigma_{12}b}{\sqrt{a'\Sigma_{11}a}\sqrt{b'\Sigma_{22}b}}=\max\limits_{|\widetilde{a}|=|\widetilde{b}|=1}{\widetilde{a}'\Sigma_{11}^{-\frac{1}{2}}}\Sigma_{12}\Sigma_{22}^{-\frac{1}{2}}\widetilde{b}$。（$\widetilde{a}=\Sigma_{11}^{\frac{1}{2}}a,\widetilde{b}=\Sigma_{22}^{\frac{1}{2}}b$）

第i对典型相关对：$\max\limits_{a_i,b_i\neq 0}\frac{a_i'\Sigma_{12}b_i}{\sqrt{a_i'\Sigma_{11}a_i}\sqrt{b_i'\Sigma_{22}b_i}}$，且满足$var(U_i)=a_i'\Sigma_{11}a_i=1,var(V_i)=b_i'\Sigma_{22}b_i=1$，且$Cov(U_i,U_k)=0$，$Cov(V_i,V_k)=0$，对于$k<i$。

$U_k=e_k'\Sigma_{11}^{-1/2}X$，$V_k=f_k'\Sigma_{22}^{-1/2}Y$，$e_k$为$\Sigma_{11}^{-1/2}\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\Sigma_{11}^{-1/2}$的单位特征向量，$f_k$为$\Sigma_{22}^{-1/2}\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}\Sigma_{22}^{-1/2}$的单位特征向量，$\rho_k^2$为第$k$大的特征值，$f_k=\frac{1}{\rho_k}\Sigma_{22}^{-1/2}\Sigma_{21}\Sigma_{11}^{-1/2}e_k$，$corr(U_k,V_k)=\rho_k$。（右上为$\Sigma_{12}$）

新加性质：$cov(U_i,V_k)=corr(U_i,V_k)=0,k\neq i$。（注意$a_i$相互不垂直）因为$cov(U,V)=A\Sigma B'$为对角元素为$\rho_k$的对角阵。$cov(U,U)=A\Sigma A'=I$，$cov(V,V)=B\Sigma_{22}B'=I$。（此处二者维数都为$p$）（$A=[a_1,a_2,...,a_p]'$，$B=[b_1,b_2,...,b_p]'$）

$corr(U,X)=A\Sigma_{11}V_{11}^{-1/2}$，$corr(U,Y)=A\Sigma_{12}V_{22}^{-1/2}$，$corr(V,Y)=B\Sigma_{22}V_{22}^{-1/2}$，$corr(V,X)=B\Sigma_{21}V_{11}^{-1/2}$。

$\max\limits_{b}corr(U_k,b'Y)=corr(U_k,V_k)=\rho_k^2=\max\limits_{a}corr(a'X,V_k)=corr(U_k,V_k)$，$\rho_k^2$为$U_k$被$Y$解释的方差比例，也为$V_k$被$X$解释的方差比例。

数据标准化（旋转平移）后，$U_i$与$V_i$和$\rho_i$都不变。

![Fo09UsIiIz6CP_422Zh_pg5zzvct](C:/Users/%E6%83%A0%E6%99%AE/Desktop/Fo09UsIiIz6CP_422Zh_pg5zzvct.jpg)

![FsfPz-d-nSGh0SubbQWLtOfZAtJL](C:/Users/%E6%83%A0%E6%99%AE/Desktop/FsfPz-d-nSGh0SubbQWLtOfZAtJL.jpg)



### 判别分析(DA)

LDA：取两个总体的方差为$s^2_y=\frac{\sum\limits_{j=1}^{n_1}(y_{1j}-\bar y_1)^2+\sum\limits_{j=1}^{n_2}(y_{2j}-\bar y_2)^2}{n_1+n_2-2}$，二者分离度为$\frac{\bar y_1-\bar y_2}{s_y}$，让分离度的平方最大，即$\max\limits_{a}\frac{(\bar y_1-\bar y_2)^2}{s_y^2}=\max\limits_{a}\frac{(a'\bar x_1-a'\bar x_2)^2}{a'S_{pooled}a}=\max\limits_a\frac{(a'd)^2}{a'S_{pooled}a}$，该最大值为$D^2=(\bar x_1-\bar x_2)'S_{pooled}^{-1}(\bar x_1-\bar x_2)$，而$a\propto S_{pooled}^{-1}(\bar x_1-\bar x_2)$。（Fisher）

ECM：假设两个分类为$\pi_1$、$\pi_2$，两个分类出现的前验概率为$p_1$、$p_2$，错判代价为$c(2|1)$、$c(1|2)$。令$ECM=c(2|1)P(实为1判为2)+c(1|2)P(实为2判为1)=c(2|1)P(2|1)p_1+c(1|2)P(1|2)p_2$最小，从而得到$R_1$与$R_2$的判定分界点：$R_1:\frac{f_1(x)}{f_2(x)}\ge(\frac{c(1|2)}{c(2|1)})(\frac{p_2}{p_1})$，$R_2$反之。（$f_1(x)$与$f_2(x)$是两个总体的已知分布）

TPM：忽略错判代价的ECM，相当于$c(1|2)=c(2|1)$。

Bayes：当$P(\pi_1|X=x)>P(\pi_2|X=x)$，则认为其应被分为$R_1$，$P(x\in \pi_1|X=x)=\frac{p_1f_1(x)}{p_1f_1(x)+p_2f_2(x)}$，其结果相当于$c(1|2)=c(2|1)$的ECM。

对两个多元正态分布的总体的分类（LDA）：①认为两个总体方差一致$\Sigma_1=\Sigma_2=\Sigma$，则分界$R_1:(\mu_1-\mu_2)'\Sigma^{-1}x-\frac{1}{2}(\mu_1-\mu_2)'\Sigma^{-1}(\mu_1+\mu_2)\ge\ln\left[(\frac{c(1|2)}{c(2|1)})(\frac{p_2}{p_1})\right]$。（对于样本数据，取$\Sigma=S_{pooled}=\frac{\sum\limits_{j=1}^{n_1}(x_{1j}-\bar x_1)(x_{1j}-\bar x_1)'+\sum\limits_{j=1}^{n_2}(x_{2j}-\bar x_2)(x_{2j}-\bar x_2)'}{n_1+n_2-2}$，$\mu_1=\bar x_1$，$\mu_2=\bar x_2$）

QDA：如果认为两个多元正态分布方差不相等，则$R_1:-\frac{1}{2}x'(\Sigma_1^{-1}-\Sigma_2^{-1})x+(\mu_1'\Sigma_1^{-1}-\mu_2'\Sigma_2^{-1})x-k\ge\ln\left[(\frac{c(1|2)}{c(2|1)})(\frac{p_2}{p_1})\right]$（其中$k=\frac{1}{2}\ln\left(\frac{|\Sigma_1|}{|\Sigma_2|}\right)+\frac{1}{2}(\mu_1'\Sigma_1^{-1}\mu_1-\mu_2'\Sigma_2^{-1}\mu_2)$）。

对于两个等方差正态分布的最小TPM为$\Phi(-\frac{\Delta}{2})$（其中$\Delta^2=a'\Sigma a=(\mu_1-\mu_2)'\Sigma^{-1}(\mu_1-\mu_2)$）

APER（显著错误比例）：尽量减小如此分类（确定界限后）后的错误比例APER，易出现过拟合（LOOCV: 对每个样本点x，用排除x的全 部数据作训练集，预测x的分类  ；n-fold CV: 将样本均分n份。对每份，用其他样本作训练集，该份作验证集。）

出现过拟合，可以采用Cross Validation：一部分数据用于训练，剩下数据用于检验

对全部数据进行整体标准化（线性变换），不影响分类结果。



### 聚类分析(MSA)

**Hierarchical Clustering**（层次聚类法）：

Agglomerative 聚类：需要定义点与点和聚类与聚类的距离（single linkage、complete linkage、average linkage、Ward's linkage、质心）（Ward' linkage：$SSE=\sum\limits_{i=1}^K\sum\limits_{x\in C_i}dist^2(m_i,x)$，即从最开始合并时，尽量让分出的块的方差最小）（不需要提前确定分类数目，容易展现层次关系，但是计算复杂度高）

![img](https://qn-st0.yuketang.cn/FjHRcW-jRjAQNf4Wfs_Sjxb5mMOo)



**Partitioning Clustering**：让每个数据被分到一个没有交叠的子集中

K-means聚类法：先给出初始K个质心，此后根据质心进行分类（离同一质心最近的分为一类），然后再重新确定各自质心，重复操作直到稳定。（需要实现确定分类数K，点点距离定义，初始质心位置）（确定K数的方法：①Elbow：让SSE不断大幅度降低，到达缓慢变化时的临界点取为K；②Empirical经验主义：$k\approx\sqrt{n/2}$；③Cross Validation：一部分分类一部分检验）（效率高，但对异常值敏感，且对奇怪形状分类有问题）

![img](https://qn-st0.yuketang.cn/FkyHBruonEog2kFnJgNjbdGke2Fr)

（可以把换为K-Modes或者K-Medoids，找到其中最接近中心的点，而非质心，这样就避免了对异常值的敏感）

![img](https://qn-st0.yuketang.cn/FoimljjwLiLOFs-ZtiVY0noOjEC0)



**Model-based 聚类法：**(soft)

Mixture Modelling：认为某种观测值可以来自很多中类别分布的交叠。每种类别有自己的概率分布和权重。一般认为各个分类都为多元正态分布。$Y\sim Multinomial(1,\pi)$，$X|Y=l\sim N(\mu_l,\Sigma_l)$。目的就是估计$\pi$概率和$(\mu_l,\Sigma_l)$。$f(x|\pi,\mu,\Sigma)=\prod\limits_{i=1}^n\sum\limits_{l=1}^k\pi_lf_l(x_i|\mu_l,\Sigma_l)$

假设已知分类Y，那么就是个分类问题：$l(\pi,\mu,\Sigma)=\sum\limits_{i=1}^n\log f_{y_i}(x_i|\mu_{y_i},\Sigma_{y_i})+\sum\limits_{i=1}^n\log P(Y_i=y_i;\pi)$，不断更新，使得$\hat\mu_l^{(t+1)}=\frac{\sum_{i=1}^n1\{y_i=l\}X_i}{\sum_{i=1}^n1\{y_i=l\}}$，$\hat\Sigma_{l}^{(t+1)}=\frac{\sum_{i=1}^n1\{y_i=l\}(X_i-\hat\mu_l^{(t+1)})(X_i-\hat\mu_l^{(t+1)})'}{\sum_{i=1}^n1\{y_i=l\}}$，$\hat\pi_l^{(t+1)}=\frac{1}{n}\sum_{i=1}^n1\{y_i=l\}$。

实际上未知$Y$，所以$\Gamma_{n\times k}=(\gamma_{il})_{i\le n,l\le k},\gamma_{il}=P(\delta_{il}=1)=P(1\{Y_i=l\})$。计算时在推测$Y$和推测$\pi,(\mu_l,\Sigma_l)$中切换。此时
